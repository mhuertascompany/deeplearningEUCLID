{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for brightness profile fitting\n",
    "\n",
    "http://adsabs.harvard.edu/abs/2018MNRAS.475..894T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import MaxPooling2D, Convolution2D, Conv2D\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET PATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Where you take the simulated data for train/validation\n",
    "pathinData = '/Users/marchuertascompany/Documents/teaching/EUCLID_school/TD/Tutorial_EUCLID/' \n",
    "#where do you wanna save the results\n",
    "pathinModel = '/Users/marchuertascompany/Documents/teaching/EUCLID_school/TD/models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Section 1: Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've saved the data that you will use in this tutorial in numpy format. The data consist of 20,000 stamps of simulated HST/CANDELS galaxies (the design matrix X) and the correspondent half light radius, i.e. the parameter that we aim to predict (stored in the target file Y). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.load(pathinData+'Stamps_Simulated_Galaxies_tutorial.npy')\n",
    "Y = np.load(pathinData+'Parameters_Simulated_Galaxies_tutorial.npy') \n",
    "\n",
    "\n",
    "#visualize the data\n",
    "i = 0\n",
    "plt.imshow(X[i,0,:,:,],clim=(0,.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scale the features\n",
    "What algorithm are you planning on using? Does it require you scale the features in any way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#=============================================== \n",
    "# Right shape\n",
    "#===============================================\n",
    "print ('X.shape= ', X.shape)\n",
    "X = np.expand_dims(X[:,0,:,:], axis=3)\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "#=============================================== \n",
    "# Scale\n",
    "#===============================================\n",
    "scaler = preprocessing.StandardScaler().fit(Y)\n",
    "Y=scaler.transform(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split into training and test sets\n",
    "\n",
    "What's a reasonable choice here for how much data should go into your test set? What are you going to do to ensure you don't overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spliting in Training and Test datasets\n",
    "X_train = X[0:len(X)//5*4,:,:,:]   \n",
    "X_val = X[len(X)//5*4:,:,:,:]\n",
    "Y_train = Y[0:len(Y)//5*4,0]\n",
    "Y_val = Y[len(Y)//5*4:,0]\n",
    "print ('Y_train.shape= ', Y_train.shape)          \n",
    "print ('Y_val.shape= ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Built a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Build_Model():\n",
    "    \n",
    "    \n",
    "    ## PARAMETERS OF THE MODEL\n",
    "    print ('Using Built Model')\n",
    "    dropoutpar = 0.15\n",
    "    img_rows=128\n",
    "    img_cols=128\n",
    "    img_channels=1\n",
    "    depth=8\n",
    "    nb_dense = 64  \n",
    "\n",
    "    # KERAS SEQUENTIAL-MODEL\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(depth, (4, 4),activation='relu', input_shape=(img_rows, img_cols, img_channels), padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropoutpar))\n",
    "\n",
    "    model.add(GaussianNoise(0.01,input_shape=( img_rows, img_cols,img_channels)))\n",
    "    model.add(Conv2D(2*depth, (3, 3),activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(4*depth, (2, 2),activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2*nb_dense))\n",
    "    model.add(Dense(nb_dense))\n",
    "    model.add(Dense(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Built a training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Fit_Model(X_train, X_val,  Y_train, Y_val,  model):\n",
    "    \n",
    "    # let's TRAIN the model using SGD + momentum  ===> you can try different loss, optimizer and and settings\n",
    "    lr=0.01  \n",
    "    decay=0   \n",
    "    momentum=0.9 \n",
    "    sgd = SGD(lr=lr, decay=decay, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "\n",
    "    #Do u wanna use data augmentation?\n",
    "    data_augmentation = False\n",
    "    #hyperparameters of the training. Try different\n",
    "    batch_size = 64\n",
    "    nb_epoch = 15\n",
    "    if data_augmentation == False:\n",
    "        print('Not using data augmentation.')\n",
    "        history = model.fit(X_train, Y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  nb_epoch=nb_epoch,\n",
    "                  validation_data=(X_val, Y_val),\n",
    "                  shuffle=True,\n",
    "                  verbose=True)\n",
    "    if data_augmentation == True:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # this will do preprocessing and realtime data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                width_shift_range=0.0,  # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.0,  # randomly shift images vertically (fraction of total height)\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=True)  # randomly flip images\n",
    "\n",
    "        # compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied)\n",
    "        datagen.fit(X_train)\n",
    "\n",
    "        # fit the model on the batches generated by datagen.flow()\n",
    "        history = model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                                batch_size=batch_size),\n",
    "                                samples_per_epoch=X_train.shape[0],\n",
    "                                nb_epoch=nb_epoch,\n",
    "                                validation_data=(X_val, Y_val),\n",
    "                                verbose=verbose)\n",
    "        \n",
    "        return model, history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BUILT model\n",
    "model = Build_Model()\n",
    "\n",
    "#train model\n",
    "model, history = Fit_Model(X_train, X_val, Y_train, Y_val, model)\n",
    "\n",
    "# do you wanna save the model?\n",
    "saveModel = False\n",
    "if saveModel == True:\n",
    "    #save scaler\n",
    "    scalerfile = pathinModel+'scaler.sav'\n",
    "    pickle.dump(scaler, open(scalerfile, 'wb'))\n",
    "    model.save(pathinModel+'model.h5')\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the performance of your DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning curves, look and the training error, validation error and the generalization error. Do we observe overfitting, underfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.epoch, history.history['loss'], label='loss')\n",
    "plt.plot(history.epoch, history.history['val_loss'], label='val_loss')\n",
    "plt.title('Training performance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best validation loss: %.3f\" % (np.min(history.history['val_loss'])))\n",
    "print(\"at: %d\" % np.argmin(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. A better metric - R^2\n",
    "The choice of the metric to evaluate the performance of our ML is a fundamental step. The choice of the metric depend on the particular Task that we aim to solve.  Here we chose the coefficient of regression R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = scaler.inverse_transform(Y_val)\n",
    "pred = scaler.inverse_transform(model.predict(X_val))\n",
    "pred = pred[:,0]\n",
    "mse=np.mean(np.square(pred-val)) \n",
    "R2 = 1. - mse/np.square(np.std(val))\n",
    "print ('R2=', R2)\n",
    "\n",
    "#and now we plot prediction versus validation value\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.title('Val sample. $R^2$ = %s' % (R2), size=11)\n",
    "plt.xlabel('Par', fontsize=13)\n",
    "plt.ylabel(\"Par Predicted\", fontsize=13)\n",
    "plt.scatter(val, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What's the best you can get?\n",
    "I can get an  R^2 of around 0.97 for this dataset. Try to see if you can beat that! Try different to modify the model  and hyperparameters. Think about optimising the hyperparameters. If you do multiple tests, ensure you always keep aside a test set to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on real data\n",
    "\n",
    "Now try yourself to test on real: \n",
    "\n",
    "1) load real dataset, \n",
    "2) test the model as it is on Real Data, \n",
    "3) load model and keep training (transfer learning) using part of the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [astrophd_tutorial]",
   "language": "python",
   "name": "Python [astrophd_tutorial]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
